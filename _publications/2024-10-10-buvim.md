---
title: "Vision mamba for classification of breast ultrasound images"
collection: publications
category: conferences
permalink: /publication/2024-10-10-buvim
status: 'Accepted'
excerpt: This paper shows that Mamba-based models outperform CNNs and ViTs on breast ultrasound datasets, achieving a 1.98% AUC and 5.0% accuracy improvement, while effectively capturing long-range dependencies.
date: 2024-10-10
venue: ' International Conference on Medical Image Computing and Computer Assisted Intervention (MICCAI), Deep-Brea3th Workshop'
#slidesurl: ''
paperurl: 'https://arxiv.org/pdf/2407.03552'
citation: 'A. Nasiri-Sarvi, M. S. Hosseini, and H. Rivaz, “Vision mamba for classification of
breast ultrasound images,” arXiv preprint arXiv:2407.03552, 2024.'
---

Mamba-based models, VMamba and Vim, are a recent family of vision encoders that offer promising performance improvements in many computer vision tasks. This paper compares Mamba-based models with traditional Convolutional Neural Networks (CNNs) and  Vision Transformers (ViTs) using the breast ultrasound BUSI dataset and Breast Ultrasound B dataset. Our evaluation, which includes multiple runs of experiments and statistical significance analysis, demonstrates that some of the Mamba-based architectures often outperform CNN and ViT models with statistically significant results. For example, in the B dataset, the best Mamba-based models have a 1.98\% average AUC and a 5.0\% average Accuracy improvement compared to the best non-Mamba-based model in this study. These Mamba-based models effectively capture long-range dependencies while maintaining some inductive biases, making them suitable for applications with limited data.